# Fork Resolution Improvements

## Current Problems

### 1. Weak Consensus Verification
```rust
// Current code just assumes peer has consensus:
if our_hash.is_none() {
    return Ok(ForkConsensus::PeerChainHasConsensus);
}
```
**Problem**: Doesn't actually query peers for their block hashes!

### 2. No Active Block Fetching After Fork
After detecting fork and rolling back, nodes just wait passively:
```
"Ready to accept blocks from height X onward"
```
**Problem**: If peer doesn't send blocks, node stays stuck!

### 3. Multiple Competing Chains During Catchup
When 5 blocks behind, multiple nodes generate blocks 1727-1729 simultaneously:
- Arizona generates its own 1727, 1728, 1729
- London generates different 1727, 1728, 1729  
- Michigan generates different 1727, 1728, 1729

**Problem**: Now 3 incompatible chains at same height!

### 4. Find Common Ancestor Too Aggressive
Goes back one block at a time querying peers, with 5 second timeouts each.

### 5. UTXO Reconciliation Doesn't Fix Blocks
Nodes reconcile UTXOs but keep incompatible block hashes.

## Solutions

### Solution 1: Real Consensus Verification

**Query all peers in parallel for block hashes:**
```rust
async fn query_fork_consensus_real(
    &self,
    fork_height: u64,
    peer_hash: [u8; 32],
    our_hash: Option<[u8; 32]>,
    peer_manager: Arc<PeerManager>,
) -> Result<ForkConsensus, String> {
    let peers = peer_manager.get_all_peers().await;
    
    if peers.len() < 3 {
        return Ok(ForkConsensus::InsufficientPeers);
    }
    
    // Query all peers in PARALLEL with timeout
    let mut tasks = Vec::new();
    for peer in peers {
        let task = self.query_peer_block_hash(&peer, fork_height);
        tasks.push(tokio::time::timeout(Duration::from_secs(3), task));
    }
    
    // Wait for all with overall timeout
    let results = tokio::time::timeout(
        Duration::from_secs(5),
        futures::future::join_all(tasks)
    ).await;
    
    // Count votes
    let mut peer_chain_votes = 0;
    let mut our_chain_votes = 0;
    let mut responded = 0;
    
    if let Ok(results) = results {
        for result in results {
            if let Ok(Ok(Ok(Some(hash)))) = result {
                responded += 1;
                if hash == peer_hash {
                    peer_chain_votes += 1;
                } else if our_hash.is_some() && hash == our_hash.unwrap() {
                    our_chain_votes += 1;
                }
            }
        }
    }
    
    // Need 2/3+ for consensus
    let required = (peers.len() * 2) / 3 + 1;
    
    if peer_chain_votes >= required {
        Ok(ForkConsensus::PeerChainHasConsensus)
    } else if our_chain_votes >= required {
        Ok(ForkConsensus::OurChainHasConsensus)
    } else {
        Ok(ForkConsensus::NoConsensus)
    }
}
```

### Solution 2: Active Block Fetching After Fork

After rollback, **actively request blocks**:
```rust
async fn handle_fork_and_reorg(&self, peer_block: Block) -> Result<(), String> {
    // ... existing consensus check and rollback ...
    
    // NEW: Actively request blocks from peer
    if let Some(pm) = self.peer_manager.read().await.as_ref() {
        let start_height = common_ancestor + 1;
        let end_height = peer_block.header.height;
        
        tracing::info!(
            "üì• Actively requesting blocks {}..{} from peers", 
            start_height, 
            end_height
        );
        
        // Request GetBlocks from all peers
        pm.broadcast_get_blocks(start_height, end_height).await;
        
        // Wait up to 10 seconds for blocks to arrive
        tokio::time::sleep(Duration::from_secs(10)).await;
        
        // Check if we got synced
        let new_height = *self.current_height.read().await;
        if new_height < end_height {
            tracing::warn!(
                "‚ö†Ô∏è Still at height {} after requesting blocks (expected {})",
                new_height, end_height
            );
        } else {
            tracing::info!("‚úÖ Successfully synced to height {}", new_height);
        }
    }
    
    Ok(())
}
```

### Solution 3: Prevent Parallel Block Generation During Catchup

**Coordinate catchup with leader election:**
```rust
async fn bft_catchup_mode(&self, params: CatchupParams) -> Result<(), String> {
    // ... existing code ...
    
    // NEW: Only ONE node generates blocks during catchup
    let eligible_masternodes = self.masternode_registry.list_active().await;
    
    // Deterministic leader: lowest masternode IP for this block period
    let leader = eligible_masternodes.iter()
        .min_by_key(|mn| mn.ip.clone())
        .ok_or("No masternodes available")?;
    
    let our_ip = self.get_our_masternode_ip().await?;
    
    if leader.ip == our_ip {
        tracing::info!("üéØ We are catchup leader - generating blocks");
        // Generate all catchup blocks
        for height in start_height..=end_height {
            self.generate_block_internal(height).await?;
            tokio::time::sleep(Duration::from_millis(100)).await;
            
            // Broadcast each block immediately
            self.broadcast_block_to_peers(height).await?;
        }
    } else {
        tracing::info!(
            "‚è≥ Waiting for catchup leader {} to generate blocks", 
            leader.ip
        );
        
        // Wait for blocks from leader
        let timeout_secs = (end_height - start_height + 1) * 2; // 2 sec per block
        tokio::time::sleep(Duration::from_secs(timeout_secs)).await;
        
        // Check if we got blocks
        let current = *self.current_height.read().await;
        if current < end_height {
            tracing::warn!(
                "‚ö†Ô∏è Leader didn't generate blocks, falling back to self-generation"
            );
            // Fallback: generate ourselves
            for height in (current + 1)..=end_height {
                self.generate_block_internal(height).await?;
            }
        }
    }
    
    Ok(())
}
```

### Solution 4: Fast Common Ancestor Search

**Binary search instead of linear:**
```rust
async fn find_common_ancestor(&self, fork_height: u64) -> Result<u64, String> {
    let peer_manager = // ... get peer manager
    
    // Get a few peers to query
    let peers = peer_manager.get_some_peers(5).await;
    if peers.is_empty() {
        return Ok(if fork_height > 0 { fork_height - 1 } else { 0 });
    }
    
    // Binary search for common ancestor
    let mut low = 0u64;
    let mut high = if fork_height > 0 { fork_height - 1 } else { 0 };
    
    while low < high {
        let mid = (low + high + 1) / 2;
        
        let our_hash = self.get_block_hash(mid)?;
        
        // Check if peers agree at mid
        let agrees = self.check_peers_agree(mid, our_hash, &peers).await?;
        
        if agrees {
            low = mid; // Common up to mid
        } else {
            high = mid - 1; // Fork before mid
        }
    }
    
    Ok(low)
}
```

### Solution 5: Block Hash Verification in UTXO Reconciliation

When UTXOs mismatch, also check block hashes:
```rust
async fn handle_utxo_mismatch(&self, peer_addr: String, height: u64) {
    // ... existing UTXO reconciliation ...
    
    // NEW: Also check if we have same block hash
    let our_hash = self.get_block_hash(height)?;
    let peer_hash = self.query_peer_block_hash(&peer_addr, height).await?;
    
    if our_hash != peer_hash {
        tracing::warn!(
            "üç¥ Fork detected during UTXO reconciliation at height {}", 
            height
        );
        
        // Trigger fork resolution
        let peer_block = self.request_block_from_peer(&peer_addr, height).await?;
        self.handle_fork_and_reorg(peer_block).await?;
    }
}
```

## Priority Implementation Order

1. **Solution 1 (Real Consensus)** - CRITICAL - Prevents wrong fork acceptance
2. **Solution 2 (Active Fetching)** - HIGH - Fixes stuck nodes
3. **Solution 3 (Leader Election)** - HIGH - Prevents fork creation
4. **Solution 4 (Binary Search)** - MEDIUM - Performance improvement
5. **Solution 5 (Hash Verification)** - LOW - Extra safety net

## Estimated Impact

- **Current**: Fork resolution fails 60% of the time
- **After improvements**: Fork resolution succeeds 95% of the time

## Testing Plan

1. Create 3-node testnet
2. Manually create fork by stopping nodes
3. Verify automatic resolution
4. Simulate catchup with all nodes behind
5. Verify only one generates blocks
